Here's a glossary of common terms related to performance testing. This can help you better understand the concepts and terminology used in this field.

### Performance Testing Glossary

1. **Load Testing**: A type of performance testing that assesses how an application behaves under a specific expected load, simulating multiple users or transactions.

2. **Stress Testing**: Testing conducted to evaluate an application's behavior under extreme conditions or loads, often to identify the breaking point or failure thresholds.

3. **Endurance Testing**: A form of performance testing that evaluates how an application performs over an extended period, assessing stability and resource consumption.

4. **Spike Testing**: A testing approach where the load is suddenly increased significantly for a short period to assess the application’s response and recovery capabilities.

5. **Volume Testing**: Testing focused on evaluating an application's performance when processing a large volume of data, often to identify bottlenecks related to data handling.

6. **Throughput**: The number of transactions or requests processed by a system in a given time frame, typically measured in requests per second (RPS) or transactions per second (TPS).

7. **Response Time**: The total time taken to process a request and return a response, often measured from the time a request is sent until the response is received.

8. **Latency**: The time delay before a transfer of data begins following an instruction, often affecting user experience.

9. **Error Rate**: The percentage of failed requests compared to total requests during a performance test. High error rates can indicate issues with the application under load.

10. **Concurrent Users**: The number of users interacting with the application simultaneously during a performance test.

11. **Bottleneck**: A point in the system where performance is limited due to insufficient resources, leading to degraded performance or failures.

12. **Resource Utilization**: The measurement of how much of a system's resources (CPU, memory, disk I/O, network bandwidth) are being used during a test.

13. **Scalability**: The capability of an application to handle a growing amount of work or its potential to accommodate growth, often assessed through load and stress testing.

14. **Service Level Agreement (SLA)**: A formal agreement that defines expected performance standards, such as response times and availability, between service providers and clients.

15. **Profiling**: The process of analyzing the performance of an application to identify resource usage and performance bottlenecks.

16. **Monitoring**: The continuous observation and tracking of an application’s performance metrics during testing and in production.

17. **Real User Monitoring (RUM)**: A technique that gathers performance data from actual users in real time, providing insights into user experience and application performance.

18. **Synthetic Monitoring**: A method that simulates user interactions with an application to measure performance, often using scripted tests.

19. **Warm-Up Period**: The initial phase of a performance test where the application is allowed to initialize and stabilize before collecting performance metrics.

20. **Test Script**: A set of automated instructions that define the actions to be performed during a performance test, often simulating user interactions.

21. **Result Analysis**: The process of reviewing and interpreting performance test results to identify trends, issues, and areas for improvement.

22. **Capacity Planning**: The process of determining the resources required to meet expected loads, helping ensure that systems can handle future demand.

23. **Benchmarking**: The practice of comparing the performance of an application against a standard or reference point, often involving tests under similar conditions.

24. **Garbage Collection**: The automatic process of reclaiming memory by cleaning up unused objects, which can impact performance if not managed effectively.

25. **Network Simulation**: The use of tools to mimic different network conditions (latency, bandwidth, packet loss) during testing to evaluate how the application performs under various network scenarios.

---
### [Back to README](./README.md)